% Created 2015-12-13 Sun 11:26
\documentclass[graduation-thesis]{mlarticle}
     \usepackage[dvipdfmx]{graphicx}
     \usepackage{url}
     \usepackage{atbegshi}
     \AtBeginShipoutFirst{\special{pdf:tounicode EUC-UCS2}}
     \usepackage[dvipdfmx,setpagesize=false]{hyperref}
          \usepackage[dvipdfmx]{color}
\usepackage{url}
\usepackage{float}
\usepackage[setpagesize=false]{hyperref}
\usepackage{ascmac}
\usepackage{here}
\usepackage{txfonts}
\usepackage{listings, jlisting}
\author{61200185 情報工学科 縣直道}
\date{\today}
\title{}
\begin{document}


\makeatletter
\renewcommand{\thetable}{
        \thesection.\arabic{table}
} %「表（章番号）-#.」と表記するための措置
\@addtoreset{table}{section}
  
\renewcommand{\thefigure}{
        \thesection.\arabic{figure}
}
\@addtoreset{figure}{section} %「図（章番号）-#.」と表記するための措置

\setcounter{page}{1}

\pagenumbering{roman}
\tableofcontents
\clearpage

\pagenumbering{arabic}

\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}
 
\lstset{
        basicstyle=\ttfamily\footnotesize, 
        frame=single,
        keywordstyle=\color{keywords},
        commentstyle=\color{comments},
        stringstyle=\color{red},
        showstringspaces=false,
        identifierstyle=\color{green},
        }

\section{はじめに}
\label{intro}
現在，仮想化技術が普及してきており，様々なサービスの基盤技術として広く用いられている．
仮想化技術の利用例として，クラウドコンピューティングやInfrastructure as a Service (IaaS) が挙げられる．
クラウドコンピューティングでは，データセンタに配置されている大量の物理マシンを，多くの利用者が同時に利用する．
IaaSは，クラウドコンピューティングの一種で，ネットワークを経由して，CPU，ストレージ，OS，ミドルウェアなどの基盤を提供するサービスである．

クラウドコンピューティングでは，ハードウェア資源を仮想化して利用者に提供することで，その物理的な構成に依らない柔軟で効率的な計算資源の活用を達成することができる．
通常，ハードウェア資源の構成を変更することは，物理的構成を変更する必要があるため，ネットワークを経由して基盤を利用するIaaSの場合，物理的構成を変更することは難しいため，困難である．
クラウドコンピューティングを利用することで，高い可用性を得ることができ，大規模な並列計算なども行いやすくなる．

クラウド環境上では，一台の仮想マシン上で，一つのアプリケーションのみを実行する場合がほとんどである．
これは，クラウド環境上では，物理マシンというリソースを増やすことなく仮想マシンを増やすことが可能であり，複数のアプリケーションを実行する場合には仮想マシンを増やすことで対処可能なためである．
ある利用者が複数のアプリケーションを実行したいと考えた場合，一つの仮想マシン内で実行するのではなく，それぞれのアプリケーションに対して仮想マシンを割り当てる．
このようにすることで，それぞれのアプリケーションが互いのパフォーマンスに影響することなく動作する．

また，このようなクラウド環境上の仮想マシンには，ほとんどの場合，物理マシン上で利用されるような汎用オペレーティングシステム(Linux，Windows，*BSD)が利用される．

汎用オペレーティングシステムは，高度な抽象化，機能を多数備えている．
オペレーティングシステム上で動作するアプリケーションが固定されている場合，それらの抽象化，機能は不要となる．

クラウド環境上の仮想マシンでは，ほとんどの場合，動作するアプリケーションが固定されている．
仮想マシンに汎用オペレーティングシステムを利用する場合，使用されることのないオペレーティングシステムの機能のために，カーネルイメージのサイズが増大し，セキュリティにも問題を抱えることになる． 
カーネルイメージのサイズが大きいと仮想マシンのサイズも大きくなり，仮想マシンの起動時間も大きくなる．
クラウド環境では，一つのアプリケーションごとに一台の仮想マシンを立ち上げるため，これらは大きな問題となる．

この問題の解決案として，あるアプリケーションに特化したオペレーティングシステムを開発するという方法が考えられる．
しかし，オペレーティングシステムの開発は難しく，誤った実装が行われた場合システム全体に影響を及ぼしてしまう．
さらに，アプリケーションごとに新たにオペレーティングシステムを開発する必要があるという問題もある．
したがって，アプリケーションごとに特化したオペレーティングシステムを開発するという手法は，現実的とは言えない．


本研究では，汎用オペレーティングシステムから，アプリケーションが必要としない機能を自動的に削除することで，アプリケーションに特化したカーネルを生成する手法を提案する．
アプリケーションのソースコードを静的解析することで，必要となるオペレーティングシステムの機能を絞り込み，ブートに必要な部分と絞り込んだ機能を実装している部分のみを残すよう，カーネルのソースコードを書き換える．
自動的にカーネルを改変することで，アプリケーションごとに特化したオペレーティングシステムを開発する必要があるという問題点を解決する．

% TODO: gcov と openstack のリファレンス
本研究の提案する手法を実現するために，まず，Linux カーネルソースコードの内，ブートに必要な部分とそうでない部分を分類する必要がある．
アプリケーションが使用する機能以外のすべてのソースコードを削除してしまうとブートすることが出来ないオペレーティングシステムになってしまう．
本研究では，この分類を行う手法を実装した．

本研究では，ブートに必要な部分を判別するため，\texttt{gcov} を用い，ブートまでのコードカバレッジを取得した．
ここから，ブートまでに一度も実行されていなかった関数を判別することができる．

ブートまでに一度も実行されていなかった関数をすべて削除したカーネルはブートすることが出来なかったため，Open Stack 上でカーネルがブートすることをテストできる環境を構築し，現在実験中である．

本論文の構成をいかに示す．
第\ref{relative}章では，本研究と関連する研究を紹介する．
第\ref{proposal}章では，本研究が提案する手法について説明する．
第\ref{implementation}章では，提案手法の実装について説明する．
第\ref{result}章では，本研究の分析結果を述べる．
第\ref{conclusion}章では，まとめと今後の課題について述べる．

\clearpage
\section {関連研究}
\label{relative}

本章では，本研究と関連する研究を紹介する．
exokernel は，極小さなカーネルに，ライブラリとして高度な機能を付加する構成を提案している．
OSv は，仮想マシン上で一つのアプリケーションのみを実行する環境に特化したオペレーティングシステムを開発した．
unikernel は，カーネルコンパイル時に，オペレーティングシステムの機能の有無を設定することができるカーネルである．
Duneは，アプリケーションに安全かつ効率的にハードウェアの情報を公開するために，仮想化支援の機構を利用している．

\subsection {Exokernel: An Operating System Architecture for Application-Level Resource Management}
\label{relative:exokernel}
カーネルのサイズを小さくするために，極単純な機能のみを持つカーネルに，Library OS として高レベルな抽象化などの機能を付加するという構成を提案している．

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=7.0cm]{images/generalos.png}
    \caption{汎用オペレーティングシステムの構成}
    \label{fig:generalos}
  \end{center}
\end{figure}
\begin{figure}[H]
  \begin{center}
    \includegraphics[width=7.0cm]{images/exokernel.png}
    \caption{exokernelの構成}
    \label{fig:exokernel}
  \end{center}
\end{figure}


中心となる極小さなカーネルは，ハードウェア資源にアクセスするための低レベルなインターフェースを提供する役割のみを持つ．
プロセス，ファイル，アドレス空間，プロセス間通信といった高レベルな抽象化は，カーネルの提供するインターフェースを用いて，すべてカーネル外のライブラリとして実装される．
このような構成にすることで，アプリケーションが必要とする抽象化のみを提供することができ，アプリケーションが直接ハードウェア資源にアクセスすることも可能となる．
また，アプリケーションが必要とする機能のみをもつオペレーティングシステムを構築できるため，オペレーティングシステムによるオーバヘッドやメモリ使用量の増加といった問題を解決することができる．

Linuxのような，汎用的なオペレーティングシステムは，あらゆるアプリケーションが必要とするすべての機能を実装することを目標としているが，汎用目的の抽象化の実装は，その抽象化を必要としないアプリケーションにとっては単なるオーバヘッドになってしまっている．
また，汎用オペレーティングシステムに組み込まれている抽象化のように，抽象化の実装が固定されていることは，アプリケーションのパフォーマンスを低下させ，アプリケーションに制限を課している．
ハードウェア資源の抽象化の実現方法は一つではなく，すべてのアプリケーションに対して最適な抽象化の実装は存在しないためである．
例えば，動的に確保されたメモリの内，不要になった領域を自動的に開放するガベージコレクションの実装では，ページングを行うハードウェアを直接扱うことで，パフォーマンスが向上する場合がある．
しかし，Linuxのような汎用オペレーティングシステムでは，カーネルが高レベルな抽象化を強制するため，ユーザ空間でのハードウェアアクセスは非常に制限されており，このような最適化を行うことが出来ない．
さらに，汎用的なカーネルでは，page faultなどの低レベルな情報を抽象化し，アプリケーションから隠蔽するため，アプリケーションからはこれらの情報にアクセスすることが出来ない．
そのため，あるアプリケーションにとって最適なハードウェア抽象化の実装を，アプリケーション側では行うことが出来ない．

ハードウェア抽象化の実装を，アプリケーションレベルのライブラリによって行うことで，アプリケーションにとって不要な抽象化を利用しないという選択が可能になるだけでなく，抽象化の実装をより柔軟にすることができる．

汎用的なカーネルにおける抽象化と異なり，ライブラリでの抽象化は，あらゆるアプリケーションにとって良い方法で実装する必要がない．
アプリケーションが異なり，最適な抽象化の方法が異なった場合には，それぞれの抽象化を別のライブラリとして実装すればよいためである．

汎用オペレーティングシステムは図\ref{fig:generalos}のように，すべてのアプリケーションやライブラリが，カーネルを通じてハードウェアにアクセスするが，exokernelでは，図\ref{fig:exokernel}のように，Library OSがハードウェアに直接アクセス可能である．
また，汎用的なカーネルは，アプリケーションはバグがあり悪意あるものも存在するという前提の元でカーネルが設計されているため，ハードウェアを抽象化する際，アプリケーションを信用しない実装にする必要がある．
一方，Library OSのデザインでは，ハードウェアを抽象化するライブラリ自体が信用されないアプリケーションレベルで実装される．
これは，安全性の面で汎用的なカーネルに劣るが，ハードウェアを抽象化する際，そのライブラリを使用するアプリケーションを完全に信用した実装をすることが可能になるというメリットがある．
Library OSのような設計を取ることで，オペレーティングシステムの大部分がアプリケーションのレベルで実行され，非常に小さなカーネルとすることができる．

Library OSシステムは，可搬性に優れたアプリケーションを構築できる．
exokernelの提供するインターフェースを，アプリケーションが直接利用することでも，汎用目的の抽象化によるオーバヘッドは削減できる．
しかし，そのような構成のアプリケーションは，それぞれのハードウェア特有のインターフェースを利用するため，可搬性に乏しい．
一方，exokernel上に実装されたLibrary OSを利用することで，利用するLibrary OSと同じインターフェースを提供するすべての環境で実行可能なアプリケーションを構築できる．

% TODO: Exokernel の，本研究との関連をうまく説明
exokernelでは，アプリケーションレベルでハードウェアを扱うことができるため，様々なデバイスへの対応もアプリケーションレベルで実装しなければならない．

\subsection {OSv - Optimizing the Operating System for Virtual Machines}
\label{relative:unikernel}
OSv は，一つのアプリケーションをクラウド環境上で実行することに特化したオペレーティングシステムである．

標準的な仮想マシンの構成を図\ref{fig:vm}に示す．

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=8.0cm]{images/vm.png}
    \caption{仮想マシンの構成}
    \label{fig:vm}
  \end{center}
\end{figure}

仮想マシンでは，ハードウェアをハイパーバイザが仮想化し，ハイパーバイザ上でオペレーティングシステムが動作する．

仮想マシン上で一つのアプリケーションのみを実行する場合，オペレーティングシステムが持つプロセス間Isolationという役割は，ハイパーバイザが担っており，不要な抽象化が重なりオーバヘッドになる．
汎用オペレーティングシステムの重要な役割として，プロセス間のIsolationと，各プロセスとカーネルのIsolationがある．
このIsolationによって，システムコールが高コストになり，コンテキストスイッチによるオーバヘッドが生じる．
また，これらの実装のために，汎用オペレーティングシステムは非常に複雑になっている．
しかし，これらの機能は，一つのオペレーティングシステム上で，複数のユーザや複数のアプリケーションが動作する場合には，必要なコストである．
一つのプロセスのバグや悪意によって，他のプロセスやカーネルが影響を受けないようにするためである．

第\ref{intro}章で述べたように，クラウド環境上の仮想マシンでは，一つのアプリケーションのみを実行する場合がほとんどである．
仮想マシン上で一つのアプリケーションのみを実行するという環境では，
アプリケーションにバグがあったり，悪意あるアプリケーションが動作した場合であっても，
仮想マシン上ではそのアプリケーションしか実行されておらず，同じ物理マシン上で動作する他の仮想マシンは，ハイパーバイザによって保護されているため影響を受けない．
したがって，仮想マシン上のゲストOSには，Isolationのための抽象化を実装する必要がない．

また，ハードウェアの抽象化が重複しているという問題点も挙げられる．
汎用オペレーティングシステムでは，アプリケーションがハードウェアに直接アクセスすることが出来ないよう，ハードウェアを抽象化するレイヤーが存在する．
しかし，仮想マシンでは，ハイパーバイザがハードウェアを抽象化して，仮想マシン上のゲストOSに見せている．
したがって，仮想マシン上で実行されるアプリケーションからは，ハードウェアが，ハイパーバイザとゲストOSそれぞれに抽象化されている．

OSvは，仮想マシン上で一つのアプリケーションのみを実行することを前提とした，ハードウェア抽象化やプロセス間のIsolationの機能を持たない，小さなカーネルを開発した．
カーネルから不要な抽象化を省くことで，カーネルのブートやアプリケーションの実行が，汎用オペレーティングシステムよりも高速になる．
仮想マシン上で実行されることを前提にしたオペレーティングシステムであるため，複数のハイパーバイザに対応しているが，アーキテクチャ依存のコードは非常に少ない．

OSvは，exokernelが提案した，Library OSのデザインをとっている．exokernelの役割をハイパーバイザが担う．
仮想化環境では，ハイパーバイザがexokernelのように，それぞれのアプリケーションからハードウェアを保護する．
ハイパーバイザ上のそれぞれの仮想マシンが，アプリケーションの役割を持つ．

OSvでは，一つのメモリ空間のみを持つようなメモリ管理方法をとる．
通常の汎用オペレーティングシステムでは，プロセスごとにメモリ空間が分けられている．これは，一つのプロセスが行うメモリの書き換えが他のプロセスに影響を及ぼさないようにするためである．
OSv上では，一つのアプリケーションのみが実行されることを想定しているため，このようなメモリ空間の分断は不要である．
また，仮想マシン間はハイパーバイザによって保護されているため，一つの仮想マシン内での変更が，他の仮想マシン上のアプリケーションに影響することはない．
OSv上では，カーネルもすべてのスレッドも一つのページテーブルを使用するため，コンテキストスイッチによるオーバヘッドが削減される．

OSvは，Posix APIとほぼ互換性のあるAPIと独自のAPIを提供している．Posix API互換のAPIでは，システムコールは単なる関数呼び出しに過ぎず，Linuxなどにおけるシステムコールが必要とする，ユーザモードから特権モードへの切り替えや引数のコピーなどは行われない．

このようにすることで，Linux上で動作していたネットワークインテンシブなアプリケーションについて，25％のスループット向上，47％のレイテンシ削減を，アプリケーションを改変することなく実現した．
また，OSv独自のネットワークAPIを使用するようアプリケーションの改変を行い，スループットが290％向上した．

OSvは，仮想マシン上で一つのアプリケーションのみを実行する環境に特化したオペレーティングシステムとなっているが，
実行するアプリケーションに特化したオペレーティングシステムを提供するものではない．
OSvは，対象とする環境により適したオペレーティングシステムであり，アプリケーションにとって不要な機能が残ってしまう場合がある．

また，仮想マシン上で一つのアプリケーションのみを実行する環境を前提としているため，forkやexecといった関数は提供されていない．
そのため，forkを使用するHTTPサーバのようなアプリケーションを実行することが出来ない．

\subsection {Unikernel: Library Operating Systems for the Cloud}
\label{relative:unikernel}
unikernelは，コンパイル時に，オペレーティングシステムが提供する機能の有効，無効を設定することができるカーネルを提案した．
OSvと同様に，仮想マシン上で一つのアプリケーションのみを実行する環境を想定し，
そのアプリケーションが必要としないオペレーティングシステムの機能を無効にしてコンパイルすることで，
カーネルのサイズを縮小し，無駄なコードを削減する．

unikernelでは，システムライブラリや言語のランタイム，アプリケーションなどがすべて，仮想マシン上でブート可能な一つのイメージにコンパイルされる．
汎用オペレーティングシステムを仮想マシン上で利用する場合との比較を図\ref{fig:unikernel}に示す．
Mirageとは，unikernelのプロトタイプ実装であり，OCamlで実装されている．

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=10.0cm]{images/unikernel.png}
    \caption{(左)汎用オペレーティングシステムにおけるソフトウェアレイヤー (右)unikernelにおけるソフトウェアレイヤー}
    \label{fig:unikernel}
  \end{center}
\end{figure}

unikernelは，OSv同様，Library OSのデザインを取っており，ハイパーバイザ上で動作する．
Library OSのデザインは，物理マシン上で動作させることが難しかった．
これは，さまざまなハードウェアデバイスに対応する必要があったためである．
一方，ハイパーバイザ上で動作する場合はこのような問題が生じない．

クラウド上にシステムをデプロイする際，設定による問題が生じる場合がある．
環境によって設定が異なるために，アプリケーションが意図通りに動作しない場合があるという問題である．
しかし，あらゆるアプリケーションに対応できる設定は存在しない．
そこで，unikernelでは，設定をすべてコンパイルプロセスに組み込むというアプローチをとっている．
データベースやWebサーバのような個別のアプリケーションを，設定ファイルによる設定を通じて統合するのではなく，すべてライブラリとして扱い，一つのアプリケーションとして扱う．
通常設定ファイルで行われるパラメータの設定は，ライブラリ関数呼び出しの動的な引数やコンパイル時の静的な値によって決定される．

unikernelは，予め設定可能になっている部分以外の設定を行うことができない．
したがって，unikernel上で無効にできるようになっていない機能を使用しないアプリケーションの場合，その機能を削除することが出来ない．

\subsection{Dune: Safe User-level Access to Privileged CPU Features}
\label{relative:dune}
Duneは，アプリケーションが，直接かつ安全にハードウェアの情報を扱うことができるようにするためのシステムである．

\ref{relative:exokernel}で述べたように，ハードウェアの情報を直接アプリケーションが利用することが出来れば，アプリケーションのパフォーマンスを向上できる場合がある．
しかし，一般には，ハードウェアの情報はカーネルが抽象化しており，アプリケーションが直接アクセスすることはできない．
また，アプリケーションが直接ハードウェアにアクセスすることができるよう，オペレーティングシステムを開発することは難しい．

そこで，仮想化がハードウェアを抽象化することができることに注目する．
Duneでは仮想化を，マシン抽象を提供するためではなく，プロセス抽象を提供するために用いる．

通常仮想化は，ハードウェアを抽象化することで，仮想的なマシンを提供するために用いられる．
OSvやunikernelが提案しているように，アプリケーションを仮想マシン上で実行し，その環境に合うよう特化したオペレーティングシステムを用いることで，安全にハードウェアへのアクセス権をアプリケーションに与えることができる．

しかし，このアプローチでは，仮想マシン上のアプリケーションと，ホストOSとの連携に問題が生じる．
単純なプロセスと異なり，仮想マシン上にあるアプリケーションは，ホストOSのファイルの読み書きやプロセスのfork，UNIX domain socketによる通信といった処理が行えないためである．

Intel VT-Xのような仮想化支援を利用することで， 安全に，かつ直接のハードウェアへのアクセス権を与えつつ，プロセスとしての性質を保っている．


\subsection{まとめ}
\label{relative:conclusion}
従来より，アプリケーションによっては汎用オペレーティングシステムに無駄が多く，また汎用オペレーティングシステムが提供する抽象化がアプリケーションの最適化の障害になるという問題を解決するための研究がなされている．
近年のクラウド環境の普及に伴い，exokernelの提案したLibrary OSのデザインがより有効になってきている．

OSvやUnikernelは仮想マシン上で一つのプロセスのみを実行するという環境を想定しており，HTTPサーバのように一つのアプリケーションがforkして複数のプロセスをもつというようなものは実行できない．
このような場合，OSvやUnikernelでは，もう一台仮想マシンを起動し，仮想マシン間通信を行う必要がある．
クラウド環境の利用目的の一つである，大規模な並列計算を行う場合，仮想マシン間の通信は大きなオーバヘッドになる．
また，これらの研究の提案手法では，予め削除可能になっている機能しか削除することが出来ないという問題点がある．

\clearpage
\section{提案}
\label{proposal}

\subsection{概要}
\label{proposal:abstruction}
本研究では，クラウド上で，一つのアプリケーションのみを実行する仮想マシンを想定し，あるアプリケーションのみを実行することに特化したオペレーティングシステムになるよう，汎用オペレーティングシステムを自動的に改変することでカーネルを軽量化する手法を提案する．
すべてのアプリケーションそれぞれに対して，そのアプリケーションが必要とする機能のみを実装したオペレーティングシステムを生成することで，
不要な抽象化機能を持つためにカーネルのサイズが増大しメモリのフットプリントが大きくなったり，セキュリティ上の脆弱性を生じる可能性が大きくなる問題を解決する．

アプリケーションを実行するためには，アプリケーションが利用する機能を実装する部分と，ブートに利用される部分が必要となる．
そこで，図\ref{fig:kernelcode}で網掛けになっている部分を削除するよう，Linuxカーネルのソースコードを改変する．

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=8.0cm]{images/kernelcode.png}
    \caption{Linuxカーネルの改変}
    \label{fig:kernelcode}
  \end{center}
\end{figure}

アプリケーションが変わってもカーネルの自動軽量化が適用できるようにするため，
アプリケーションのソースコードを静的解析し，そのアプリケーションが必要とするオペレーティングシステムの機能を絞り込む．

また，アプリケーションが必要とする機能を実装する部分以外にも，ブートに必要な部分は削除しないようにする必要がある．
そこで，まず，カーネルソースコードを，ブートに必要な部分とそうでない部分に分類する必要がある．

本研究では，関数単位での削除のみを対象とし，Linuxカーネルを改変する．
関数単位の削除とは，必要でないと判断できる関数の定義を，panic関数を呼び出すだけの関数になるよう，ソースコードを書き換えることである．
ソースコード\ref{code:before}，\ref{code:after}に例を示す．

\begin{lstlisting}[language=C, caption=ftruncate関数(改変前), label=code:before]
  SYSCALL_DEFINE2(ftruncate, unsigned int, fd, unsigned long, length)
  {
    return do_sys_truncate(path, length);
  }
\end{lstlisting}
\begin{lstlisting}[language=C, caption=ftruncate関数(改変後), label=code:after]
  SYSCALL_DEFINE2(ftruncate, unsigned int, fd, unsigned long, length)
  {
    #ifdef _LINUX_KERNEL_H
    panic("Execute eliminated function(ftruncate).");
    #endif
  }
\end{lstlisting}

\subsection{静的解析}
\label{proposal:static}
Linuxでは，オペレーティングシステムの機能はすべて，システムコールとしてアプリケーションに提供される．
オペレーティングシステムの機能をアプリケーションから利用するには，必ずシステムコールを呼び出す必要がある．
つまり，アプリケーションが呼び出しているシステムコールが分かれば，どの機能が必要であるか判別することができる．

そこで，アプリケーションのソースコードを静的解析し，コールグラフを生成する．
アプリケーションのコールグラフから，システムコールの呼び出しを発見する．
アプリケーションが利用しているライブラリについても同様の解析を行い，システムコールの呼び出しを発見する必要がある．

また，カーネルのソースコードも静的解析を行い，コールグラフを生成する．
システムコールとしてアプリケーションに提供されているインターフェースとなる関数が，その内部で呼び出している関数を再帰的に発見する必要がある．
アプリケーションの静的解析によって必要であるとわかったシステムコールが，その内部で必要としている関数も，削除しないようにする必要があるためである．

コールグラフを用いた解析は，関数単位での解析となる．
コールグラフを用いることで，カーネルソースコード内のすべての関数について，必要か不要かを判定する．

アプリケーションがシステムコールを発行する際，システムコールに引数として渡す値が固定されているという場合がある．
例えば，開かれたファイルディスクリプタに関する操作を行うfcntlシステムコールのシグネチャはソースコード\ref{fcntlsig}のようになっている．

\begin{lstlisting}[language=C, caption=fcntlシステムコールのシグネチャ, label=fcntlsig]
  int fnctl(int fd, int cmd, ... /* arg */);
\end{lstlisting}

第二引数には26種類のフラグを指定することができる．
fnctlには，指定された値によって，様々な処理を行う．
\texttt{F\_DUPFD}が指定された場合，第一引数で指定されたファイルディスクリプタを複製する．
\texttt{F\_GETFD}が指定された場合，第一引数で指定されたファイルディスクリプタに関連するフラグを読みだす．

fcntlは，引数に渡された値のチェックを行った後，do\_fcntl関数を呼び出す．
ソースコード\ref{fcntlimpl}にdo\_fcntlの実装の一部を抜粋する．

\begin{lstlisting}[language=C, caption=do\_fcntlの実装(抜粋), label=fcntlimpl]
static long do_fcntl(int fd, unsigned int cmd, unsigned long arg,
                struct file *filp)
{
        long err = -EINVAL;

        switch (cmd) {
        case F_DUPFD:
                err = f_dupfd(arg, filp, 0);
                break;
        case F_DUPFD_CLOEXEC:
                err = f_dupfd(arg, filp, O_CLOEXEC);
                break;
        case F_GETFD:
                err = get_close_on_exec(fd) ? FD_CLOEXEC : 0;
                break;
        case F_SETFD:
                err = 0;
                set_close_on_exec(fd, arg & FD_CLOEXEC);
                break;
\end{lstlisting}

fcntlは第二引数の値に応じて関数を呼び出し，処理の本体はその呼び出す関数が行うようになっている．
ここで，アプリケーションにおけるfcntlのすべての呼び出しについて，指定されることがないフラグがあれば，そのフラグを処理する部分は不要になる．
さらに，fcntlの第二引数に指定される値が常に一定であることがわかった場合，ソースコード\ref{fcntlimpl}の，switch文そのものを削除することが可能である．

このような最適化を行うためには，アプリケーションコードとカーネルコード，双方について，コントロールフローの解析を行う必要がある．

アプリケーションのコントロールフロー解析は，システムコールに渡す引数の取り得る値を列挙するために行う．
例えば，ソースコード\ref{exapp}のような，ファイルの内容をコピーするだけの単純なアプリケーションについて考える．

\begin{lstlisting}[language=C, caption=アプリケーションの例, label=exapp]
    char buf[1024];
    int from_fd = open(argv[1], O_RDONLY);
    read(from_fd, buf, 1024);
    int open_flg = O_WRONLY;
    if (argc > 2 && strcmp(argv[2], "-c") == 0) {
      open_flg |= O_CREAT;
    }
    int to_fd = open("app.txt", open_flg);
    write(to_fd, buf, 1024);
    close(from_fd);
    close(to_fd);
\end{lstlisting}

このアプリケーションのコントロールフローグラフは，図\ref{fig:controlflow}のようになる．

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=6.0cm]{images/controlflow.eps}
    \caption{アプリケーションのコントロールフローグラフ例}
    \label{fig:controlflow}
  \end{center}
\end{figure}

この時，openシステムコールに渡される第二引数はopen\_flgは，O\_WRONLYかO\_WRONLY $|$ O\_CREAT のどちらかであることがわかる．
このような場合，O\_RDONLYやO\_RDWRを処理するコードはすべて不要であり，削除することができる．

カーネルコードのコントロールフロー解析は，アプリケーションの解析で得られたシステムコールにの引数になり得る値についての情報を利用し，switch文中の不要なcase節や不要なif文を識別するために行う．

\subsection{カーネルのブート}
\label{propo:boot}
カーネルのソースコードには，システムコールとしてアプリケーションに提供される関数に関連するコード以外にも，多くの関数が含まれている．
デバイスを認識し初期化するコードや，マウント処理などである．
マシンがブートする際，カーネルはハードウェアやメモリ，ファイルシステムの初期化を行い，initプロセスを起動する．
したがって，アプリケーションが必要とするシステムコールに関連するコード以外のすべてのコードを削除してしまうと，そのカーネルはブートすることができなくなってしまう．

そこで，カーネルを改変する際，ブートに必要な部分を改変しないようにする必要がある．
そのために，カーネルコードの内，ブートに必要な部分とそうでない部分を分類する．

カーネルのソースコードでは \_\_initというマクロが利用されている．
これは，kernel/include/linux/init.hとkernel/include/linux/compiler.hで，ソースコード\ref{code:__init}のように定義されている．

\begin{lstlisting}[language=C, caption=\_\_initマクロの定義, label=code:__init]
  #define __init __section(.init.text) __cold notrace
  #define __section(S) __attribute__ ((__section__(#S)))
\end{lstlisting}

これは，関数を修飾し，その関数が初期化処理用の関数であることを宣言している．
\_\_initで修飾された関数は，.init.textというセクションに配置される．
.init.textセクションは初期化が終了すると廃棄されるメモリ領域である．
すなわち，\_\_initで修飾された関数はすべてブートに必要な関数であると考えられる．

また，Linuxカーネル内のinit/ディレクトリも同様に初期化に必要な処理を担当するソースコードを含んでいるため，これらもブートに必要な関数であると考えられる．

一方，これら以外にも，ファイルシステムの初期化処理など，カーネルがブート時に実行するべきコードは，Linuxカーネルソース内に広く存在している．

そこで，ブートに必要な部分の分類には，カーネルコードのカバレッジを利用する．
マシンを起動し，カーネルがブートするまでのカバレッジを取得し，そのカバレッジ情報から，ブートまでに実行されていることがわかったコードは，ブートに必要であると判断できる．

しかし，カーネルのブートステップは非常に複雑であり，ブートまでのカバレッジ情報から分類した，ブートまでに実行されていることがわかったコードのみを残すよう，カーネルを改変しても，ブートすることが出来なかった．
そこで，本研究では，ブートに必要な関数を分類するために，一関数ずつブートするかどうかをテストする手法を提案する．

\subsection{Linuxカーネル}
\label{propo:linux}

\subsubsection{概要}
\label{propo:linux:abstruction}
カーネルは，システムの起動時にメモリにロードされ，ハードウェア資源の管理やプロセスのスケジューリングなど，オペレーティングシステムの中心的な処理を行うプログラムである．
Linuxシステムでは一般的に，/boot/ディレクトリ以下にvmlinuxやvmlinuzを名前に含むファイルに格納され，ブート時にブートローダによってメモリにロードされる．
vmlinuxは圧縮されていないカーネルである．
vmlinuzは，zlibアルゴリズムによって圧縮されたカーネルを意味し，ブートローダはこれを展開してからメモリにロードする必要がある．
また，Linuxカーネルの発展に伴うカーネルサイズの増加に対応するために，より制限の強いアーキテクチャ上であっても，展開しロードが可能なbzImageというフォーマットが作られた．
カーネルがメモリ上にロードされる際には，フォーマットに依らず，すべて展開されたvmlinuxとしてロードされる．

また，システム起動後に，必要に応じてメモリ上にロードされるモジュールという仕組みを持っており，これらは/lib/modules/ディレクトリ以下に配置される．

Linuxカーネルは，カーネルやモジュールの構成を変更するために，カーネルコンフィグレーションが可能になっている．
カーネルコンフィグレーションを行うことで，カーネルの機能の変更・追加・削除や，デバイスの変更への対応が可能となる．

Linuxカーネルのコンフィグレーションは，カーネルソースコードのルートディレクトリにある，.configというファイルを編集することで行う．

\subsubsection{コンパイル}
\label{propo:linux:compile}
Linuxカーネルは，ビルド時に設定ファイルを読み込むことで，様々な設定を反映したカーネルを生成する．


Linuxカーネルのコンパイルは非常に複雑なものになっている．
Linuxカーネルのソースコードには，カーネル本体のためのソースコード以外にも，カーネルイメージの圧縮や，設定ファイルや環境に合わせたコンパイルを行うためのソースコードが含まれている．

例えば，Linuxカーネル内では，アーキテクチャに依存する値や設定ファイルに応じた値，コンパイルする環境に応じた値を使用する必要がある．
単純なアーキテクチャ依存の値は，arch/ディレクトリ以下の，それぞれのアーキテクチャ用のサブディレクトリ内で定義されているため，特殊な処理は不要だが，設定と関連する値などは，コンパイル時に生成しなければならない．
そこで，Linuxカーネルのコンパイルでは，まず設定ファイルを読み込み，設定依存，環境依存の値をマクロで定義するヘッダファイルをinclude/generated/ディレクトリに生成している．
実際にビルド時に生成されるファイルの一つである，include/generated/bounds.hをソースコード\ref{code:generated}に示す．

\begin{lstlisting}[language=C, caption=include/generated/bounds.h, label=code:generated]
  /*
  * DO NOT MODIFY.
  *
  * This file was generated by Kbuild
  */

  #define NR_PAGEFLAGS 25 /* __NR_PAGEFLAGS       # */
  #define MAX_NR_ZONES 4 /* __MAX_NR_ZONES        # */
  #define NR_CPUS_BITS 8 /* ilog2(CONFIG_NR_CPUS) # */
  #define SPINLOCK_SIZE 4 /* sizeof(spinlock_t)   # */

  #endif
\end{lstlisting}

include/generated/bounds.hには，環境に依存する値を定義するマクロが記述される．



% TODO: 実際の例をファイル名，関数名レベルで上げる．圧縮とgeneratedがわかりやすいはず
また，Linuxカーネルは通常，gzipを用いて圧縮されている．ブート時のセットアップで，解凍されメモリ上に展開される．
ビルドの過程で，コンパイルされたLinuxカーネルを圧縮し，カーネルイメージを生成する．
Linuxカーネルを圧縮・解凍するためのコードも，ソースコード内のlib/zlib\_deflate/，lib/zlib\_inflate/に含まれている．
したがって，アプリケーションが必要としていない，ブート時にも実行されないコードであっても，単純に削除することは出来ない．


それぞれのソースコード内でも特殊な記述がされている場合が多い．
通常，拡張子.cのファイルはC言語ソースコードであり，他のC言語ソースコードからインクルードされるのは，拡張子.hのヘッダファイルである．
しかし，Linuxカーネル内では，C言語ソースコードが他のC言語ソースコードをインクルードしている例がある．
kernel/bounds.cがこれに該当する．
ソースコード\ref{code:bounds}にkernel/bounds.cの内容を示す．

\begin{lstlisting}[language=C, caption=kernel/bounds.c, label=code:bounds]
/*
 * Generate definitions needed by the preprocessor.
 * This code generates raw asm output which is post-processed
 * to extract and format the required data.
 */

#define __GENERATING_BOUNDS_H
/* Include headers that define the enum constants of interest */
#include <linux/page-flags.h>
#include <linux/mmzone.h>
#include <linux/kbuild.h>
#include <linux/log2.h>
#include <linux/spinlock_types.h>

void foo(void)
{
        /* The enum constants to put into include/generated/bounds.h */
        DEFINE(NR_PAGEFLAGS, __NR_PAGEFLAGS);
        DEFINE(MAX_NR_ZONES, __MAX_NR_ZONES);
#ifdef CONFIG_SMP
        DEFINE(NR_CPUS_BITS, ilog2(CONFIG_NR_CPUS));
#endif
        DEFINE(SPINLOCK_SIZE, sizeof(spinlock_t));
        /* End of constants */
}
\end{lstlisting}

このファイル内では，関数fooの本体内で定数マクロを定義している．
本研究で提案する手法は，関数を削除するものであるため，関数内で定義したマクロもすべて削除されてしまう．
kernel/bounds.cが定義する定数は，このファイルをインクルードするファイルで利用されるものであるため，これらのマクロが削除されるとカーネルをコンパイルすることができない．

さらに，インラインアセンブラも多用されている．
Linuxカーネルは，非常に低レベルな処理を行う必要があるため，C言語レベルで表現できない場合がある．
例えば，プロセススケジューラ内で，実行コンテキストを切り替える処理について述べる．
切り替え前のプロセスのメモリ空間を，切り替え先のプロセスのメモリ空間に切り替え，CPUのレジスタの値を退避・復帰させる必要がある．
これを行うためには，CPUのレジスタを書き換える必要があるため，アセンブラでの記述が必要になる．
このような処理を行うために，Linuxカーネルではインラインアセンブラを利用する．
インラインアセンブラを使用しているコードに，インラインアセンブラ内で関数を定義するプログラムを記述している箇所がある．
その一つがarch/x86/kernel/kprobes/core.c内のjprobe\_return関数である．
ソースコード\ref{code:inlineasm}にその定義を示す．

\begin{lstlisting}[language=C, caption=jprobe\_return関数, label=code:inlineasm]
void jprobe_return(void)
{
        struct kprobe_ctlblk *kcb = get_kprobe_ctlblk();

        asm volatile (
#ifdef CONFIG_X86_64
                        "       xchg   %%rbx,%%rsp      \n"
#else
                        "       xchgl   %%ebx,%%esp     \n"
#endif
                        "       int3                    \n"
                        "       .globl jprobe_return_end\n"
                        "       jprobe_return_end:      \n"
                        "       nop                     \n"::"b"
                        (kcb->jprobe_saved_sp):"memory");
}
\end{lstlisting}

インラインアセンブラ内で，jprobe\_return\_endという関数を定義している．
このようなインラインアセンブラ内での関数定義を含む関数を削除してしまうと，リンク時にエラーになってしまう．


\subsubsection{ブート}
\label{propo:linux:boot}

\ref{propo:linux:abstruction}で述べたように，Linuxカーネルは圧縮された形式でファイルに置かれる．
x86アーキテクチャでは，デフォルトではbzImage形式にされたカーネルイメージが生成される．
bzImageは，setup.binとvmlinux.binというバイナリを連結したものである．
setup.binはカーネルイメージの先頭に存在し，エントリーポイント\_startを含む．
このエントリーポイントから開始されるコードによって，圧縮されたカーネルを展開するルーチンが起動される．
vmlinux.binは，圧縮されたカーネル本体と，その展開ルーチンを含むバイナリである．

展開されたカーネルへと処理がうつった後，ハードウェアの極低レベルな初期化処理や，メモリ管理の開始が行われる．
このステップまでステップでは，その処理のほとんどがアセンブラで記述されており，本研究での改変の影響を受けにくい．
本研究では，C言語レベルでの関数単位での改変のみを行っているためである．

ハードウェアの極低レベルな初期化処理やメモリ管理が開始された後，setup\_kernel関数が呼び出される．
この関数はC言語で記述されており，initプロセスを起動するまでのカーネル環境の初期化を行う．
setup\_kernel関数はまず，環境依存の初期化処理を行う関数を呼び出す．
その後，メモリアロケータの初期化，スケジューラの初期化・起動，割り込みの処理，デバイスドライバの初期化，ファイルシステムの初期化，initプロセスの起動準備を行う．

Linuxカーネルのブートは，環境やタイミングによって実行される関数が異なる．
ハードウェアの認識，初期化処理は，ブートするマシンの利用しているハードウェアに応じて異なる処理になる．

本研究では，カーネルコードカバレッジを用いてブートに必要な部分の識別を行うことを提案しているが，
ブートの極初期の段階では，カバレッジ情報を書き込むファイルシステムのセットアップが終わっておらず，
その段階でしか実行されないコードは正しく分類できないと考えられる．
そのため，ブートに必要となる部分だけを正確に分類することは非常に難しい．


\clearpage
\section{実装}
\label{implementation}
\subsection{概要}
\label{implementation:abstruction}
提案手法を実装するためには，第\ref{proposal}章で述べた通り，まず，Linuxカーネルのソースコードの内，ブートに必要となる部分を識別する必要がある．
本研究では，Linuxカーネルのブートに必要となる部分を識別する方法を，三種類の手法で実装した．

Linuxカーネルのコードカバレッジを用いて，ブートに不要であると思われる関数名を取得する．
次に，Linuxカーネルのソースコードを構文解析し，ブートに不要であると思われる関数を削除する．

手法1では，コードカバレッジから，ブート時に実行されていたことがわかった関数以外のすべての関数を削除した．
手法2では，コードカバレッジから，ブート時に一度も実行されていなかった関数をすべて削除した．
手法3では，コードカバレッジから，ブート時に一度も実行されていなかった関数を，一つずつ削除し，ブート可能なカーネルを生成できるかテストする環境を構築した．


\subsection{ブートに必要な部分の識別}
\label{implementation:boot}
ブートに必要な部分を識別するためには，Linuxカーネルのコードカバレッジを利用する．
コードカバレッジとは，プログラムのソースコード全体に対する実行されたコードの割合を示し，通常はソフトウェアのテストがプログラム全体の内，どれだけのコードをテストできているかを示す指標として使われる．
Linuxブート直後のコードカバレッジを解析し，一度も実行されていないコードは，ブートに不要な関数であると判定できる．

コードカバレッジの取得には，gcovを用いる．
gcovとは，GNU CCと合わせて使用する，プログラムのコードカバレッジを取得するツールである．
\texttt{gcc -coverage main.c}のようにコンパイルすると，\texttt{main.gcno}というファイルと実行ファイルが生成される．
生成された実行ファイルを実行すると，\texttt{main.gcda}というファイルが生成され，\texttt{gcov main.c}とコマンドを実行するとプロファイル結果が出力される．
プロファイル結果の例をソースコード\ref{code:gcov}に示す．

\begin{lstlisting}[caption=gcovの実行結果例, label=code:gcov]
        -:    0:Source:main.c
        -:    0:Graph:main.gcno
        -:    0:Data:main.gcda
        -:    0:Runs:1
        -:    0:Programs:1
        1:    1:int f(void)
        -:    2:{
        1:    3:  return 0;
        -:    4:}
        -:    5:
        1:    6:int g(int x)
        -:    7:{
        1:    8:  if (x > 0)
    #####:    9:    return x;
        -:   10:  else
        1:   11:    return -x;
        -:   12:}
        -:   13:
    #####:   14:int h(void)
        -:   15:{
    #####:   16:  return 0;
        -:   17:}
        -:   18:
        1:   19:int main(void)
        -:   20:{
        1:   21:  int x = f();
        1:   22:  int y = g(x);
        1:   23:  return 0;
        -:   24:}
\end{lstlisting}

$:$の右側に，対象となるソースコードが，
$:$の左側に，その行が何度実行されたかが示されている．
ソースコード\ref{code:gcov}の例であれば，関数gのif文の分岐の片方が実行されていないことがわかる．
また，関数hが一度も実行されていないことがわかる．

gcovは，Linuxカーネルと組み合わせることが可能で，Linuxカーネルのビルド設定を変更し，gcovを組み込むことで，カーネルコードのカバレッジが取得できる．
本研究では，Linuxマシンをブートさせた直後のコードカバレッジを利用することで，Linuxカーネルコードのどの部分がブートに必要であるかを判定する．
ブート直後のコードカバレッジで，一度も実行されていないコードはブートプロセスにとって不要であると言える．

Linuxカーネルにgcovを組み込むためには，カーネルのビルド設定をソースコード\ref{code:gcovkernel}のように設定する．

\begin{lstlisting}[caption=gcovを組み込むLinuxカーネルの設定, label=code:gcovkernel]
  CONFIG_DEBUG_FS=y
  CONFIG_GCOV_KERNEL=y
  CONFIG_GCOV_PROFILE_ALL=y
\end{lstlisting}

Linuxカーネルのカバレッジを取得するためには，debugfsをマウントする必要がある．
debugfsとは，Linuxカーネルの内部情報をユーザが参照するための，メモリ上に存在するファイルシステムで，\texttt{mount -t debugfs /sys/kernel/debug}コマンドを実行することでマウントできる．
このようにdebugfsをマウントすると，/sys/kernel/debug/gcov/以下にLinuxカーネルのカバレッジ情報を保持するファイルとソースコードとの対応をとるためのファイルへのシンボリックリンクが生成される．
ソースコードとの対応を取るためのデータファイルであるgcnoファイルの実態は，Linuxカーネルをビルドしたディレクトリ以下に配置されている．
また，/sys/kernel/debug/gcov/resetというファイルも生成されており，このファイルに書き込みを行うと，カバレッジ情報をリセットすることができる．

本研究では，gcovのカバレッジ情報から，関数単位での実行回数を取得する．
\texttt{gcov}コマンドのオプションで，\texttt{-f}を指定すると，関数単位でのプロファイル結果が出力される．
ソースコード\ref{code:gcov}で示したプログラムについて，\texttt{gcov -f main.c}を実行した際の出力を，ソースコード\ref{code:gcovfunction}に示す．

\begin{lstlisting}[caption=関数単位でのプロファイル結果, label=code:gcovfunction]
% gcov -f main.c
Function 'f'
Lines executed:100.00% of 1

Function 'g'
Lines executed:75.00% of 4

Function 'h'
Lines executed:0.00% of 1

Function 'main'
Lines executed:100.00% of 3

File 'main.c'
Lines executed:77.78% of 9
main.c:creating 'main.c.gcov'
\end{lstlisting}

ソースコード\ref{code:gcovfunction}が示すように，関数名と，その関数本体の行数に対する実行された行数の割合が表示される．
ソースコード\ref{code:gcovfunction}から，関数hが一度も実行されていないという結果が得られる．

Linuxカーネルのブート時に一度も実行されない関数を取得するために，ブート直後のカバレッジ情報を用いる．
その際，ブート時には実行されないが，デーモンを起動するために実行されたり，デーモンが実行するコードを，ブートに必要なコードであると判定することがないよう，ブート直後に起動するデーモンはすべて無効にしておく必要がある．
Linuxカーネル内の，すべてのソースコードに対して，\texttt{gcov -f}コマンドを実行し，出力結果をパースして，実行された割合が0.00％だった関数名をすべて取得する．

\subsection{関数の削除}
\label{implementation:function}
gcovを用いて得た，ブートに不要と思われる関数を，Linuxカーネルから削除するために，Linuxカーネルのソースコードを構文解析し，得られた関数の定義されている行と列を調べる必要がある．
Linuxカーネルの構文解析には，clangの提供するLibToolingを用いる．

clangとは，C言語，C++，Objective-C，Objective-C++用のコンパイラフロントエンドである．バックエンドにはLLVMを採用している．
clangは，これらのプログラミング言語で書かれたソースコードを，構文解析し，抽象構文木に変換する．
変換された抽象構文木には，コンパイルの構文解析以降でプログラムの誤りを検知した際のエラー通知のために，ソースコード中の位置情報が含まれている．

clangの提供するLibToolingというライブラリは，clangをベースにしたツールを支援するためのライブラリである．
これを利用することで，clangに構文解析を任せることができ，抽象構文木に変換されたソースコードにアクセスすることができる．
本研究では，C言語で記述されたソースコードをclangに構文解析させ，そのソースコード内で定義されている関数の名前と，それぞれの関数の定義されているソースコード上の位置情報を取得するために，LibToolingを利用する．

gcovによるプロファイル結果をパースし，ブートに不要と思われる関数を，その関数の定義されているファイル名と合わせて出力する．
ファイル名と関数名から，LibToolingを用いて実装したツールを用いて，ファイル名，関数本体の開始位置，関数本体の末尾位置を取得する．
ソースコード\ref{code:gcov}の例の関数hの場合，7行目1列目が開始位置で，12行目1列目が末尾位置という結果が得られる．

得られた開始位置と末尾位置の間のコードを，ソースコード\ref{code:before}，ソースコード\ref{code:after}に示したように書き換える．
関数の定義そのものをすべて削除してしまうと，カーネルのコンパイルがエラーになってしまう．
そこで，関数の定義自体は残し，関数の先頭ですぐにpanic関数を呼び出すように変更する手法を取る．
panic関数は，Linuxカーネルをパニックさせる関数である．Linuxカーネルがパニックすると，Linuxカーネルは，パニックしたというメッセージと，パニックした原因，パニック時のダンプ情報の一部を出力する．
panic関数の引数には，パニックした原因として出力されるメッセージを指定する．
ソースコード\ref{code:after}に示すように，panic関数の引数には，改変対象となっている関数の定義されているファイル名と，その関数の名前を含む文字列を指定する．
ソースコード\ref{code:after}のようにpanic関数を呼び出すことで，改変したカーネルが削除した関数を呼び出してしまった場合に，その場で適切なエラーメッセージを表示するようパニックさせることができる．

clangにC言語ソースコードを構文解析させる際，カバレッジ情報から取得した関数名を持つ関数の定義を発見できない場合がある．
コードカバレッジから解析した，ブート時に一度も実行されていない関数名の一部を表\ref{table:unusedfunctions}に示す．

\begin{table}[H]
\begin{center}
  \begin{tabular}{ll} \hline
    ファイル名 & 関数名 \\ \hline \hline
    ipc/shm.c & shm\_add\_rss\_swap.isra.15 \\
    ipc/shm.c & sysvipc\_shm\_proc\_show \\
    kernel/sysctl.c & warn\_sysctl\_write.isra.1 \\
    kernel/sysctl.c & proc\_do\_large\_bitmap \\
    kernel/sysctl.c & proc\_do\_cad\_pid \\ \hline
  \end{tabular}
  \caption{ブート時に実行されていなかった関数(一部)}
  \label{table:unusedfunctions}
\end{center}
\end{table}

表\ref{table:unusedfunctions}に示されているように，gcovの出力するカバレッジ情報には，C言語の関数名として有効でない関数名を持つ関数が数多く出力されている．
"関数名\_isra.XXX"という形式になっているものは，Linuxカーネルのビルドに用いたC言語コンパイラのgccが，関数を最適化する際に，".\_isra.XXX"という接尾語を関数名に付与したものである．
最適化によって関数が書き換えられる際に，関数名が変更される．
このような場合，C言語のソースファイルを構文解析しても，関数を見つけることが出来ない．
C言語のソースファイル上で定義されている関数名は，".\_isra.XXX"という接尾語が付与される前のものである．

構文解析によってその関数の定義を発見できなかった場合，その関数の削除は行えない．
しかし，gccの最適化によって生成された関数は，元となった関数自体が削除されたり，その最適化を行う原因となった関数呼び出しを行っている関数が削除された場合，生成されなくなる．
したがって，このような関数を削除できないことは，問題にはならない．

\subsection{手法1}
\label{implementation:1}

手法1は，コードカバレッジから，ブートに必要であるとわかった関数以外のすべての関数を削除する手法である．
\ref{implementation:boot}，\ref{implementation:function}で述べた方法で，コードカバレッジからブートに必要であるとわかった関数以外のすべての関数を削除した．
ソースコード\ref{code:impl1}に擬似コードを示す．

\begin{lstlisting}[language=ruby, caption=手法1の擬似コード, label=code:impl1]
  # ブートに必要な関数名の集合
  necessary_functions = {}

  # ブートに必要な関数名一覧をコードカバレッジから取得する
  for file in Linuxカーネルのカバレッジ情報をもつすべてのファイル
    for function in fileがカバレッジ情報を持つすべての関数
      if function の実行率が0％でない
        neccesary_functions << function
      end
    end
  end

  # ブートに必要な関数以外をすべて削除する
  for file in Linuxカーネルソース内の拡張子が.cであるすべてのファイル
    for function in file内で定義されているすべての関数 
      if function が neccesary_function に含まれない
        function をpanic関数呼び出しへ置き換える
      end
    end
  end
\end{lstlisting}

この手法で改変したLinuxカーネルのビルドを試みたところ，ビルドすることが出来なかった．
\ref{propo:linux:compile}で述べたように，Linuxカーネルのビルドは複雑なステップを経る．
手法1では，ビルド時に必要となるツールや，それらのためのライブラリに必要なコードをすべて削除してしまう．
これらのコードは，カーネルのビルドが終了した時点で不要になり，カーネルそのものには組み込まれないコードも多い．
したがって，ブート時には実行されておらず，削除しても良いと判定してしまっていた．



\subsection{手法2}
\label{implementation:2}
手法2は，コードカバレッジから，ブートに不要であるとわかった関数のみをすべて削除する手法である．

手法2では，gcovによって得られるコードカバレッジが，Linuxカーネル本体に組み込まれている関数のプロファイル結果しか出力していないことに着目した．
手法2の擬似コードをソースコード\ref{code:impl2}に示す．

\begin{lstlisting}[language=ruby, caption=手法2の擬似コード, label=code:impl2]
  for file in Linuxカーネルソース内の拡張子が.cであるすべてのファイル
    if file のカバレッジ情報が出力されている
      for function in file内で定義されているすべての関数
        if functionの実行率が0％である
          functionの定義をpanic関数呼び出しへ置き換える
        end
      end
    end
  end
\end{lstlisting}

コードカバレッジの解析結果から，ブート時に一度も実行されていないとわかった関数は，必ずLinuxカーネル本体に組み込まれている．
一方で，ビルド時にのみ利用されるようなツールのためのソースコードはLinuxカーネル本体には組み込まれないため，プロファイル結果が出力されない．
そこで，ソースコード\ref{code:impl2}に示すように，カバレッジ情報が出力されたファイルについてのみ書き換えを行い，カバレッジ情報が出力されなかったファイルについては改変を行わない．

手法1で，カーネルのビルドに失敗してしまう原因を，ビルド時のエラーメッセージから調査したところ，エラーの要因となりうる箇所が非常に多く，事前に発見するのが困難であることがわかった．
そこで，手法2のように，Linuxカーネル本体に組み込まれている関数のみを対象に改変を行う方法を取った．

この手法で改変したLinuxカーネルは，ビルドすることができなかったが，手法1と異なり，ビルドステップの多くの部分は問題なく成功していたため，ビルドが失敗する原因をすべて調査することができた．
関数定義内のインラインアセンブラで関数を定義している関数や，カーネルの圧縮展開のためにビルド時にもカーネル本体にも使用される関数が原因であることがわかったため，これらを書き換えないよう変更したところ，カーネルのビルドに成功した．

しかし，このカーネルを用いたブートは不可能だった．
ブートに失敗しても，どの部分の改変が問題となっているかがわかるよう，関数を削除する際には，ソースコード\ref{code:after}が示すようにpanic関数を呼び出すように改変を加えていたが，実際にはパニックメッセージを確認することが出来なかった．
また，ブートに失敗する原因を調査するため，Linuxカーネルのパラメータ設定を用い，パニックが発生した場合自動的にリブートするよう設定したが，リブートすることもなかった．
これは，ブートの非常に初期の段階で問題が発生しているために，パニックメッセージの出力やリブートが正常に行えなかったと考えられる．


\subsection{手法3}
\label{implementation:3}
手法3は，ブート時に不要と思われる関数それぞれについて，panic関数への置き換えとブート可能かどうかのテストを行う手法である．
擬似コードを，ソースコード\ref{code:impl3}にしめす．

\begin{lstlisting}[language=ruby, caption=手法3の擬似コード, label=code:impl3]
  # ブート時に不要と思われる関数名の集合
  unneccesary_functions = {}

  # ブートに不要な関数名一覧をコードカバレッジから取得する
  for file in Linuxカーネルのカバレッジ情報をもつすべてのファイル
    for function in fileがカバレッジ情報を持つすべての関数
      if function の実行率が0％である
        unneccesary_functions << function
      end
    end
  end

  # ブート時に不要と思われる関数それぞれについて，
  # 改変とブートのテストを行う．
  for function in unneccesary_functions
    file = functionが定義されているファイル
    # ファイルのバックアップを取る
    backupfile = file + "backup"
    copy(from: file, to: backupfile)

    functionの定義をpanic関数呼び出しへ置き換える

    if カーネルのビルドが失敗
      # バックアップから復元
      move(from: backupfile, to: file)
      # 次の関数へ
      continue
    end

    if カーネルのブートに失敗
      # バックアップから復元
      move(from: backupfile, to: file)
      # 次の関数へ
      continue
    end
  end
\end{lstlisting}

手法2の分析結果から，ブートに必要な関数をすべて正確に取得することが非常に困難であることがわかる．
コードカバレッジでは，実行率0％という結果になっている関数であっても，カーネルブートの極初期の段階で実行されていて，カバレッジ結果に反映できていないと思われる．
gcovでは，\ref{implmentation:boot}で述べたように，デバッグファイルシステムにカバレッジ情報を書き込むため，デバッグファイルシステムの初期化が正常に終了する前の段階での実行は，記録されていないと思われる．
そこで，手法3では，手法1，手法2の結果を踏まえて，カーネルがブートすることをテストしながら，関数を削除していく方法を取った．

カーネルのブートの成否は，一台のマシンではテストすることができない．
ブートに失敗した場合，通常はパニックが発生するが，パニックが発生した後，その結果を通知する方法がない．
また，本研究でテストしたカーネルは，パニックを正常に発生させることができるという保証がない．
そこで，本研究では，仮想マシンを二台用いて，一方でもう一方を監視するという手法をとる．

マシンを二台用いたカーネルのテストには，ktest.plというperlで記述されたスクリプトを用いる．
ktest.plは，Linuxカーネルソースツリーに含まれるスクリプトで，設定ファイルを基にカーネルのテストを行う．
ktest.plは，リモートでの操作によってリブートできるマシンがあり，そのマシンのコンソール出力をリモートシリアルコンソールで読むことができる環境で使用することができる．
ビルドやブートが正常に行えるかをテストすることができる．

仮想マシンの構成では，OpenStackというクラウド基盤ソフトウェアを用いる．
OpenStackは，オープンソースソフトウェアのクラウド基盤ソフトウェアで，高信頼なプライベートクラウド基盤を構成することができる．
OpenStackを用いて構成したクラウド基盤上に，ブートテスト用の仮想マシンとビルド用の仮想マシンを用意する．カバレッジの解析とカーネルのビルドをビルド用マシンで行い，生成されたカーネルをブートテスト用マシンに転送する．
OpenStackを用いて構成した実装環境を図\ref{fig:openstackenv}に示す．

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=10.0cm]{images/openstackenv.png}
    \caption{OpenStackを用いて構成した実装環境}
    \label{fig:openstackenv}
  \end{center}
\end{figure}

通常，ktest.plを利用する場合は，そのスクリプト本体を書き換えることなく，設定ファイルによって，テストの内容を変更できる．
本研究では，OpenStack上でktest.plを使用するために，ktest.plをOpenStackに合わせるよう書き換える必要がある．
ktest.plは，ブートテスト用のマシンのコンソール出力を読み込み，ログインプロンプトが出力されていればブートに成功したと判定する．
OpenStack上の仮想マシンのコンソール出力は，リモートシリアルコンソールが利用できないため，ktest.plは，テスト用マシンの出力を読み込むことが出来ず，ブートに成功したかどうかを判定することが出来ない．
そこで，OpenStackクラウド上のリソースの管理のためのCompute APIをコマンドラインから操作する，novaクライアントを用いる．
novaクライアントを用いることで，ブートテスト用マシンのコンソールの出力を，ktest.plが読み取ることが可能になる．
また，改変したカーネルのブートの成否にかかわらず，ブート確認の後，ブートテスト用マシンを通常のカーネルを用いてリブートする必要がある．
このリブートには，rebootコマンドは使用できない．ブートに失敗している可能性があるためである．
ブート確認後のリブートにも，novaクライアントを用いる．

図\ref{fig:openstackenv}のように，OpenStack，ktest.pl，novaクライアントを用いて構成した環境で，実際にLinuxカーネルの改変を進めたところ，正しくブートの可否の判定が行えることがわかった．

\clearpage
\section{分析結果}
\label{result}

Linuxカーネルのブートまでのコードカバレッジをgcovを用いて解析したところ，カーネルソースコードの大部分が，ブートまでには実行されないことがわかった．
\ref{implementation:impl3}で示した，OpenStackを用いたテスト環境上で，ブート直後のカバレッジから，一度も実行されていない関数を解析したところ，コードカバレッジが出力されていた関数が56462個あり，その内41370個の関数が一度も実行されていないことがわかった．
したがって，ブートができる最小のカーネルには，通常のLinuxカーネルの約73％の関数が不要であるといえる．

\ref{implementation:impl3}で示した，OpenStack上に構築した二台の仮想マシンによる，ブートの自動テストと自動改変は，非常に時間がかかる．
これは，一つの関数を書き換えるたびに，カーネルのビルド，カーネルの転送，ブートの確認を行う必要があるためである．
ただし，カーネルのビルドは，関数を改変したファイルに依存するファイルのみを再コンパイルしている．
一つの関数を書き換え，ブートの可否を確認するために，現在の環境では，約6分かかる．
41370個の関数それぞれについてこのテストを行う必要があるため，この自動改変には膨大な時間が必要である．

しかし，Linuxカーネルのブートに必要なコードは，そのオペレーティングシステム上で動作するアプリケーションに依らず，固定されている．
本研究では，アプリケーションを変更しても自動的にLinuxカーネルをアプリケーションに特化するよう改変することを提案しているが，ブートに必要な部分の解析は，アプリケーションを変更するたびに行う必要がない．
アプリケーションを変更する場合には，そのアプリケーションが必要とするオペレーティングシステムの機能を再度解析するだけで，自動改変を適用できる．
したがって，Linuxカーネルのブートに必要なコードの解析は一度だけ行えば良い．

一方，環境が変わる場合やLinuxカーネルのコンフィグレーションを変更する場合には，Linuxカーネルのブートに必要なコードの解析を再度行う必要がある．
\ref{propo:linux:boot}に示したように，Linuxカーネルのブートプロセスは環境に大きく依存しているためである．

\section{おわりに}
\label{conclusion}
\subsection{まとめ}
仮想化技術を応用した，IaaS環境の利用が普及してきている．
IaaS環境を用いることで，ハードウェア資源の物理的構成にとらわれず，柔軟な計算資源の利用が可能になる．
IaaS環境下では，一台の仮想マシン上で一つのアプリケーションのみを実行する場合がほとんどであるが，
一方で，仮想マシン上のオペレーティングシステムには，汎用オペレーティングシステムが採用される．
汎用オペレーティングシステムは，様々なアプリケーションや利用形態に対応するため，多くの機能を持っているが，
一つのアプリケーションのみを実行する環境の場合，そのアプリケーションが必要としない機能はすべて不要である．
オペレーティングシステムは，不要な機能を持つことによって，カーネルのサイズの増大，メモリ使用量の増加が発生し，セキュリティ上の問題も生じる．

本研究では，アプリケーションを静的解析し，そのアプリケーションが必要とするオペレーティングシステムの機能のみを実装するよう，Linuxカーネルを自動的に改変する手法を提案した．
アプリケーションを静的解析し，コールグラフを生成することで，システムコールの呼び出しを発見する．
呼び出しているシステムコールから，必要とするオペレーティングシステムの機能を絞り込むことができる．
また，Linuxカーネルのブートに必要なコードを改変しないようにする必要がため，コードカバレッジを用いて分類を行う．

現在，仮想マシンを二台用いて，カーネルの改変とブートのテストを自動的に行う環境を構築し，稼働中である．

\subsection{今後の課題}

本研究では，Linuxカーネルのブートに必要なコードの分類のための手法を実装した．
Linuxカーネルをアプリケーションに特化するよう自動的に改変するためには，アプリケーションの静的解析とカーネルの静的解析が必要である．

また，第\ref{result}章で述べたように，Linuxカーネルのブートに必要なコードの分類にかかる時間は非常に長い．
現在稼働中の環境では，一組のビルド用マシンとブートテスト用マシンを用いているが，このテストは並列に実行することができる．
ビルド用マシンとブートテスト用マシンの組を複数用意し，テストを行う関数を分散させることで，高速化することができる．


\clearpage
\section{謝辞}
\label{sec-8}
\clearpage

\bibliographystyle{jplain}
\bibliography{reference}

\end{document}
